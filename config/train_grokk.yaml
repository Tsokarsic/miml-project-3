defaults:
  - dataset: mod_sum_dataset
  - model: grokk_model
  - _self_

optimizer: AdamW
#直接在这里更改优化器，保持名称与torch.optim后面的函数名称一致
#可在所使用模型对应优化器的optimizer_status中设置相关参数
using_SAM:
#输入ASAM添加sharpness aware优化方法（与优化器结合使用）,否则默认不使用
rho: 1
#sharpness的比例参数
init_method: default
#这里填入：default/orthogonal/xavier/kaiming，以选取不同的初始化方法


dataset:
  frac_train: 0.2
  p: 97
  num_p: 2
  seed: None
  #注；在更改num_p时，请同时更改grokk_model.yaml中mlp_config中的num_p,保持两者相同
train_transformer:
    num_workers: 0
    bsize: 512
    eval_every: 100
    eval_batches: 8

    max_steps: 1e5

    lr_decay:
      iter_number: 100
      #后续模型的参数注释相同
      #每这些步后更新一次，默认与eval_every相同
      method: linear
      #optional：linear(1/t衰减），quadratic（1/sqrt（t）衰减），exp（e^-t)衰减，这里t表示到第t个更新状态
      rate: 0
      #衰减参数中的beta
      warmup_steps: 10

    AdamW_status:
      lr: 1e-3
      weight_decay: 1
      betas: [0.9, 0.98]

    Adam_status:
      lr: 1e-3
      weight_decay: 1
      betas: [ 0.9, 0.98 ]

    SGD_status:
      lr: 1e-2
      weight_decay: 0.0
      momentum: 0.99
      nesterov: True

    RMSprop_status:
      lr: 0.01
      alpha: 0.98
      eps: 1e-08
      weight_decay: 0
      momentum: 0
      centered: False






train_mlp:
  num_workers: 0
  bsize: 512
  eval_every: 100
  eval_batches: 8
  max_steps: 1e5

  lr_decay:
    iter_number: 100
    method: linear
    rate: 0
    warmup_steps: 10

  AdamW_status:
    lr: 5e-4
    weight_decay: 1.0
    betas: [ 0.9, 0.98 ]

  Adam_status:
    lr: 1e-3
    weight_decay: 1
    betas: [ 0.9, 0.98 ]

  SGD_status:
    lr: 1e-2
    weight_decay: 0.0
    momentum: 0.99
    nesterov: True

  RMSprop_status:
    lr: 0.01
    alpha: 0.99
    eps: 1e-08
    weight_decay: 0
    momentum: 0
    centered: False




train_lstm:
  num_workers: 0
  bsize: 512
  eval_every: 100
  eval_batches: 8
  max_steps: 1e5

  lr_decay:
    iter_number: 100
    method: linear
    rate: 0.01
    warmup_steps: 10

  AdamW_status:
    lr: 1e-3
    weight_decay: 0.0
    betas: [ 0.9, 0.98 ]

  Adam_status:
    lr: 1e-3
    weight_decay: 1
    betas: [ 0.9, 0.98 ]

  SGD_status:
    lr: 1e-2
    weight_decay: 1.0
    momentum: 0.1
    nesterov: True

  RMSprop_status:
    lr: 0.01
    alpha: 0.99
    eps: 1e-08
    weight_decay: 0
    momentum: 0.9
    centered: False


wandb:
  use_wandb: true
  wandb_project: grokking_replica

use_edge_popup: false
sparsity: 0.9
max_edge_popup_steps: 10000
accuracy_threshold: 1.0
post_threshold_wait_steps: 500
#忽略这些超参数即可